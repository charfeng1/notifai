# GPU 性能报告

## 硬件使用情况

### 测试设备
| 项目 | 规格 |
|------|------|
| **设备型号** | Google Pixel (ARM64) |
| **CPU** | Qualcomm Snapdragon (ARMv8.2 + DotProd) |
| **GPU** | Adreno (未使用，纯 CPU 推理) |
| **RAM** | 8GB |
| **存储** | 128GB |

### 资源占用

| 指标 | 数值 | 说明 |
|------|------|------|
| **模型大小** | 475 MB | Qwen3-0.6B Q5_K_M 量化 |
| **内存占用（推理时）** | ~1.0 GB | 包含 KV Cache |
| **内存占用（空闲）** | ~200 MB | 模型已加载但未推理 |
| **CPU 利用率（推理时）** | 70-80% | 4 线程并行 |
| **电池消耗** | ~5% / 100 次分类 | 估算值 |

### 当前架构说明

**重要**：本项目当前使用 **CPU 推理**，未启用 GPU/Vulkan 加速。选择 CPU 的原因：

1. **兼容性优先** - CPU 推理兼容所有 Android 设备，Vulkan 支持参差不齐
2. **性能已足够** - 多架构优化后达到 22.79 tok/s，满足实时分类需求
3. **开发时间限制** - Vulkan 集成需要额外 1-2 周开发

**后续计划**：在验证 CPU 方案可行后，将探索 Vulkan GPU 加速以进一步提升性能。

---

## 性能指标

### 推理延迟

| 指标 | 优化前 | 优化后 | 提升 |
|------|--------|--------|------|
| **P50 总延迟** | ~72,000 ms | **1,300 ms** | **55x** |
| **P90 总延迟** | ~75,000 ms | **1,600 ms** | **47x** |
| **P99 总延迟** | ~80,000 ms | **2,500 ms** | **32x** |

### 详细性能分解

| 阶段 | 耗时 | 占比 | 说明 |
|------|------|------|------|
| **Prefill（首次）** | 1,594 ms | 64% | 处理 203 tokens（系统提示 + 用户消息） |
| **Prefill（缓存）** | 380 ms | 29% | 仅处理 35 tokens（用户消息） |
| **Decode** | 877 ms | 68% | 生成 20 tokens @ 22.79 tok/s |
| **JNI 开销** | ~100 ms | 8% | Kotlin ↔ Native 数据传输 |

### 吞吐量

| 指标 | 数值 | 说明 |
|------|------|------|
| **Prefill 速度** | 85.6 tok/s | 批量处理 prompt tokens |
| **Decode 速度** | 22.79 tok/s | 自回归生成速度 |
| **Time to First Token** | 340 ms | 用户感知延迟 |
| **端到端分类速度** | ~46 通知/分钟 | 假设每条 1.3 秒 |

### 与基线对比

| 配置 | Decode 速度 | 总延迟 | 状态 |
|------|------------|--------|------|
| 通用编译（无优化） | 1.0 tok/s | 72 秒 | ❌ 不可用 |
| 单架构优化 | ~8 tok/s | ~9 秒 | ⚠️ 勉强可用 |
| 多架构 + DotProd | 22.79 tok/s | 1.3 秒 | ✅ 生产就绪 |
| + KV Cache | 22.79 tok/s | **1.3 秒** | ✅ **当前版本** |

---

## 优化技术

### 1. 多架构构建（23x 提升）

编译 6 种针对不同 CPU 特性优化的库：

| 库变体 | 目标指令集 | 适用设备 |
|--------|-----------|----------|
| `llama_jni` | 通用 ARM64 | 所有设备（回退） |
| `llama_jni_v8` | ARMv8 + NEON | 2016+ 设备 |
| `llama_jni_v8_2` | ARMv8.2 + FP16 | 2018+ 设备 |
| `llama_jni_v8_2_dotprod` | + DotProd | Pixel 3+, 高通 855+ |
| `llama_jni_v8_2_i8mm` | + I8MM | 最新旗舰 |
| `llama_jni_v8_2_dotprod_i8mm` | DotProd + I8MM | Pixel 6+, 高通 8 Gen1+ |

**运行时检测**：
```kotlin
// 读取 /proc/cpuinfo 检测 CPU 特性
val hasDotProd = cpuFeatures.contains("asimddp")
val hasI8mm = cpuFeatures.contains("i8mm")
// 动态加载最优库
System.loadLibrary(optimalLibraryName)
```

### 2. 编译器优化

```cmake
target_compile_options(${target_name} PRIVATE
    -O3                    # 最高优化级别
    -DNDEBUG               # 禁用调试断言
    -fvectorize            # SIMD 自动向量化 ← 关键
    -ffp-model=fast        # 快速浮点运算 ← 关键
    -flto                  # 链接时优化 ← 关键
    -ffunction-sections    # 死代码消除
    -fdata-sections        # 死数据消除
)
```

### 3. KV Cache 系统提示缓存（50% 提升）

| 阶段 | 优化前 | 优化后 | 提升 |
|------|--------|--------|------|
| 预填充 tokens | 203 | 35 | -83% |
| Prefill 延迟 | 1,594 ms | 380 ms | -76% |
| 总延迟 | 2,500 ms | 1,300 ms | -48% |

**原理**：
- 系统提示（文件夹描述、优先级定义）在每次分类中相同
- 首次推理缓存系统提示的 KV 状态
- 后续推理复用缓存，只处理变化的用户消息

### 4. 量化（模型大小优化）

| 量化方案 | 模型大小 | 速度影响 | 质量影响 |
|----------|----------|----------|----------|
| FP16（原始） | ~1.2 GB | 基线 | 基线 |
| Q8_0 | ~650 MB | +10% | -1% |
| **Q5_K_M（选用）** | **475 MB** | **+20%** | **-2%** |
| Q4_K_M | ~400 MB | +25% | -5% |

选择 Q5_K_M 作为速度和质量的平衡点。

---

## 优化效果对比

### 优化前后对比图

```
延迟对比（毫秒，越低越好）
                    优化前          优化后
总延迟        ████████████████████  72,000 ms
                    ██  1,300 ms  (-98%)

Prefill       ████████████████████  ~60,000 ms
                    ██  380 ms    (-99%)

Decode        ████████████████████  ~12,000 ms
                    ████  877 ms  (-93%)
```

### 与同类方案对比

| 方案 | 模型 | Decode 速度 | 框架 |
|------|------|------------|------|
| PocketPal (llama.rn) | Qwen3-0.6B Q8 | 15.89 tok/s | React Native |
| **NotifAI（本项目）** | **Qwen3-0.6B Q5_K_M** | **22.79 tok/s** | **原生 Kotlin** |
| MLC-LLM | Qwen3-0.6B | ~18 tok/s | Vulkan |

**优势分析**：
- 比 PocketPal 快 43%（原生实现无 JS 桥接开销）
- 比 MLC-LLM 快 27%（CPU 优化充分）
- Q5_K_M 量化比 Q8 计算量更小

---

## 能耗效率（估算）

| 场景 | 能耗 | 说明 |
|------|------|------|
| 单次分类 | ~0.05% 电量 | 1.3 秒推理 |
| 100 次分类 | ~5% 电量 | 日常使用量 |
| 待机（模型加载） | ~0.1%/小时 | 内存占用，CPU 空闲 |

**优化建议**（后续实现）：
- 批量处理：累积多条通知后一次性分类
- 懒加载：仅在需要时加载模型
- 模型卸载：长时间无分类后释放内存

---

## 测试方法

### 性能测试脚本

```bash
# 查看推理性能日志
adb logcat -s LlamaPerf:I

# 示例输出
LlamaPerf: === PREFILL: 35 tokens in 379.9 ms (92.1 tok/s) ===
LlamaPerf: === DECODE: 20 tokens in 877.4 ms (22.79 tok/s) ===
LlamaPerf: === TOTAL: 1339.2 ms ===
```

### 内存监控

```bash
# 查看应用内存占用
adb shell dumpsys meminfo com.notifai
```

### CPU 利用率

```bash
# 查看 CPU 使用情况
adb shell top -n 1 | grep notifai
```

---

## 结论

| 成果 | 数值 | 意义 |
|------|------|------|
| **23x 推理加速** | 1→22.79 tok/s | 从不可用到流畅 |
| **50% 缓存优化** | 2.5s→1.3s | 接近实时响应 |
| **475MB 模型** | 60% 压缩 | 手机可接受大小 |
| **1GB 内存** | 可控占用 | 不影响其他应用 |

**关键技术贡献**：
1. 多架构 ARM 优化库的 CMake 构建方案
2. KV Cache 系统提示缓存的 JNI 实现
3. 运行时 CPU 特性检测和动态库加载

**后续优化方向**：
1. Vulkan GPU 加速（预期 1.5-2x 提升）
2. 投机解码（小模型预测减少 decode 时间）
3. 更激进的量化方案（Q4 或 INT4）
