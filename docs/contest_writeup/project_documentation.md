# NotifAI 详细项目文档

> **项目背景**：本项目在发现比赛仅剩 48 小时后启动（1月6日发现，1月7日开始开发，1月8日完成），在 **24 小时内**完成了从数据生成、模型微调到 Android 应用部署的全流程。

---

## 2.1 项目背景与价值

### 问题陈述

现代智能手机用户每天平均收到 **46-80 条通知**，其中大量是促销、系统提示等低优先级内容。重要消息（家人紧急联系、工作事故报警）常常淹没在信息洪流中，导致：

1. **信息焦虑** - 用户被迫频繁查看手机，打断工作流
2. **重要消息遗漏** - 真正紧急的通知被忽略
3. **隐私风险** - 现有云端分类方案需要上传通知内容，存在数据泄露风险
4. **网络依赖** - 云端方案在离线环境无法工作

### 市场/需求分析

| 痛点 | 普遍性 | 现有方案 | 问题 |
|------|--------|----------|------|
| 通知过载 | 全球 40 亿+ 智能手机用户 | 手动勿扰模式 | 需要用户主动设置，无智能判断 |
| 隐私泄露 | 高度敏感人群（企业、政府） | 云端 AI 分类 | 通知内容上传第三方服务器 |
| 离线场景 | 飞行、偏远地区、国际旅行 | 无 | 云端方案失效 |

**目标用户画像:**
- 信息过载的手机用户（主流）
- 注重隐私的用户（企业员工、政府人员）
- 数字游民/远程工作者
- AI 开发者（作为端侧部署参考）

### 解决方案概述

**NotifAI** 是一款完全端侧运行的智能通知分类应用：

1. **端侧 LLM 推理** - 使用微调的 Qwen3-0.6B 模型，完全在手机本地运行
2. **隐私优先** - 通知内容永不离开设备，零网络请求
3. **智能分类** - 自动将通知分入 4 个文件夹 + 3 级优先级
4. **极致优化** - 多架构编译 + KV 缓存，实现 23x 性能提升

---

## 2.2 技术架构

### 技术栈

| 层级 | 技术 | 说明 |
|------|------|------|
| **前端** | Jetpack Compose + Material 3 | 现代 Android UI 框架 |
| **后端** | Kotlin + Coroutines | 异步处理，响应式架构 |
| **AI 推理** | llama.cpp (C++) + JNI | 高性能端侧 LLM 推理 |
| **模型** | Qwen3-0.6B-notifai + LoRA 微调 | 0.6B 参数，Q8_0 量化（~650MB） |
| **备选量化** | Q5_K_M | ~475MB，1GB 内存，适合生产环境 |
| **数据库** | Room + SQLite | 本地通知存储 |
| **依赖注入** | Hilt | 解耦架构 |
| **部署环境** | Android 8.0+ (API 26+) | 覆盖 95% 设备 |

### 系统架构图

```
┌─────────────────────────────────────────────────────────────┐
│                      Android App Layer                       │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────┐  │
│  │  HomeScreen     │  │ FolderDetail    │  │ Settings    │  │
│  │  (Compose UI)   │  │ Screen          │  │ Screen      │  │
│  └────────┬────────┘  └────────┬────────┘  └──────┬──────┘  │
│           │                    │                   │         │
│  ┌────────▼────────────────────▼───────────────────▼──────┐  │
│  │                    ViewModels                          │  │
│  │   HomeViewModel  │  FolderDetailVM  │  SettingsVM      │  │
│  └────────────────────────────┬───────────────────────────┘  │
│                               │                              │
│  ┌────────────────────────────▼───────────────────────────┐  │
│  │                    Use Cases                           │  │
│  │   ClassifyNotificationUseCase  │  ManageFoldersUseCase │  │
│  └────────────────────────────┬───────────────────────────┘  │
│                               │                              │
├───────────────────────────────┼──────────────────────────────┤
│                      Domain Layer                            │
│  ┌────────────────────────────▼───────────────────────────┐  │
│  │              LlamaClassifier                           │  │
│  │   ┌─────────────┐  ┌──────────────┐  ┌──────────────┐  │  │
│  │   │PromptBuilder│  │ClassParser   │  │KV Cache Mgr  │  │  │
│  │   └──────┬──────┘  └──────┬───────┘  └──────┬───────┘  │  │
│  └──────────┼────────────────┼─────────────────┼──────────┘  │
│             │                │                 │             │
├─────────────┼────────────────┼─────────────────┼─────────────┤
│             │          JNI Layer               │             │
│  ┌──────────▼────────────────▼─────────────────▼──────────┐  │
│  │                    llama_jni.cpp                       │  │
│  │   nativeInit() │ nativeCacheSystemPrompt() │ nativeInference() │
│  └────────────────────────────┬───────────────────────────┘  │
│                               │                              │
├───────────────────────────────┼──────────────────────────────┤
│                      Native Layer (C++)                      │
│  ┌────────────────────────────▼───────────────────────────┐  │
│  │                    llama.cpp                           │  │
│  │   ┌─────────────┐  ┌──────────────┐  ┌──────────────┐  │  │
│  │   │ Model Load  │  │ Tokenizer    │  │ Sampler      │  │  │
│  │   │ (GGUF)      │  │ (BPE)        │  │ (Greedy)     │  │  │
│  │   └─────────────┘  └──────────────┘  └──────────────┘  │  │
│  └────────────────────────────────────────────────────────┘  │
│                                                              │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │  Multi-Arch Libraries (Runtime Selection)              │ │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌───────────┐  │ │
│  │  │ Generic  │ │ ARMv8    │ │ DotProd  │ │ I8MM      │  │ │
│  │  │ Fallback │ │ + NEON   │ │ Optimized│ │ (Latest)  │  │ │
│  │  └──────────┘ └──────────┘ └──────────┘ └───────────┘  │ │
│  └─────────────────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                   System Services                            │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────────────┐      ┌─────────────────────────┐   │
│  │ NotifaiListenerSvc  │ ───▶ │ ClassificationService   │   │
│  │ (NotificationListener)│     │ (Foreground Service)   │   │
│  └─────────────────────┘      └─────────────────────────┘   │
│            │                            │                    │
│            ▼                            ▼                    │
│  ┌─────────────────────────────────────────────────────────┐│
│  │                    Room Database                        ││
│  │   FolderEntity  │  NotificationEntity  │  SettingsEntity││
│  └─────────────────────────────────────────────────────────┘│
└──────────────────────────────────────────────────────────────┘
```

### 核心功能模块

#### 1. 通知监听模块 (`NotifaiListenerService`)
- 使用 Android `NotificationListenerService` API 监听系统通知
- 过滤自身通知和系统服务通知
- 提取通知元数据（应用名、标题、正文、时间戳）
- 启动分类服务处理通知

#### 2. AI 分类引擎 (`LlamaClassifier`)
- 加载量化后的 Qwen3-0.6B-notifai 模型 (Q8_0: 650MB, 或 Q5_K_M: 475MB)
- 多架构动态库选择（6 种 ARM 优化变体）
- KV Cache 系统提示缓存（减少 83% 预填充 tokens）
- 贪婪解码生成 JSON 输出

#### 3. 提示工程模块 (`PromptBuilder`)
- 动态生成系统提示（包含用户自定义文件夹）
- 支持个人偏好指令注入
- 缓存哈希检测，智能失效

#### 4. 自定义文件夹模块
- 支持添加/编辑/删除自定义文件夹
- 文件夹描述注入 LLM 系统提示
- 涌现泛化能力：无需重训练即可分类到新文件夹

#### 5. 智能通知分发 (`NotificationDispatcher`)
- **高优先级 (High)** - 立即推送，震动提醒
- **中优先级 (Medium)** - 队列聚合，每 30 分钟批量推送
- **低优先级 (Low)** - 静默处理，仅存储不打扰

#### 6. 通知存储模块 (`NotificationRepository`)
- Room 数据库持久化
- Flow 响应式数据流
- 按文件夹聚合统计

---

## 2.3 开源相关信息

- **开源协议**：Apache 2.0
  - 选择理由：允许商业使用，同时保护贡献者，与 llama.cpp 兼容
- **代码仓库地址**：https://github.com/charfeng1/notifai
- **开源状态**：
  - [x] 已开源
  - [ ] 评审后开源
  - [ ] 部分模块开源

**开源内容包括：**
- Android 应用完整源码
- llama.cpp JNI 集成代码
- 多架构 CMake 构建配置
- 合成训练数据集 (16K 条)
- 模型微调脚本
- 性能基准测试报告

---

## 2.4 开发进度

### 已完成功能

| 功能 | 状态 | 说明 |
|------|------|------|
| 通知监听与提取 | ✅ | 支持所有 Android 应用通知 |
| 端侧 LLM 推理 | ✅ | Qwen3-0.6B Q5_K_M 量化 |
| 多架构优化 | ✅ | 6 种 ARM 库变体，23x 性能提升 |
| KV Cache 缓存 | ✅ | 50% 后续推理加速 |
| 4 文件夹分类 | ✅ | Job, Private, Deals, Notices |
| 3 级优先级 | ✅ | Low, Medium, High |
| 自定义文件夹 | ✅ | 添加/编辑/删除，涌现分类 |
| 通知列表 UI | ✅ | Material 3 设计 |
| 深度链接 | ✅ | 点击跳转原应用 |

### 待完善功能

| 功能 | 优先级 | 说明 |
|------|--------|------|
| GBNF 语法约束 | 中 | 强制 JSON 输出格式 |
| Vulkan GPU 加速 | 低 | 利用移动 GPU |
| 通知历史搜索 | 中 | 全文搜索 |
| 批量重分类 | 低 | 用户纠错后批量更新 |

### 模型迭代过程

由于时间紧迫（24 小时内完成），我们采用快速迭代策略：

#### 第一次尝试：FunctionGemma-270M
- 文件夹准确率：90.6% ✅
- 优先级准确率：54.6% ❌
- 中/高优先级严重混淆（22.9% / 35.9%）

**问题**：270M 参数模型无法区分"今天处理"和"立即处理"的细微语义差异。

#### 第二次尝试：Qwen3-0.6B（最终选择）
- 文件夹准确率：94.0% ✅（+3.4%）
- 优先级准确率：83.0% ✅（+28.4%）
- 中/高优先级区分能力大幅提升

**成功原因**：更大模型（600M vs 270M）+ 简单 JSON 输出格式 + `/no_think` 禁用思考模式。

### 合成数据生成

使用 **Claude Code** 生成 16,000 条高质量训练数据：

| 维度 | 实现 |
|------|------|
| 多语言 | 30-40% 中文（微信、飞书、钉钉、抖音） |
| 多应用 | 25+ 应用，真实 Android 包名 |
| 优先级分布 | P1:P2:P3 ≈ 45%:28%:27%（避免全部紧急） |
| 质量保证 | 自动化验证 + 人工抽检 |

**为什么选择 Claude Code**：高质量中英文生成、2 小时完成、可控分布。

### 遇到的挑战与解决方案

#### 挑战 1：端侧推理速度慢（1 tok/s）
**问题**：初始实现使用通用编译，推理速度仅 1 tok/s，用户体验极差。

**解决方案**：
1. 研究 llama.rn 多架构构建策略
2. 实现 6 种 ARM 优化库变体（ARMv8, DotProd, I8MM）
3. 运行时 CPU 特性检测，动态加载最优库
4. 添加关键编译器标志（`-fvectorize`, `-flto`, `-ffp-model=fast`）

**结果**：性能提升 23x，达到 22.79 tok/s。

#### 挑战 2：重复预填充浪费计算
**问题**：每次分类都重新处理 184 tokens 的系统提示，占推理时间 60%+。

**解决方案**：
1. 拆分系统提示和用户消息
2. 首次推理缓存系统提示的 KV Cache
3. 后续推理复用缓存，只处理用户消息 (35 tokens)
4. 哈希检测系统提示变化，智能失效

**结果**：后续推理时间减少 50%，TTFT 从 1600ms 降至 380ms。

#### 挑战 3：自定义文件夹不生效
**问题**：用户添加自定义文件夹后，通知仍分类到默认文件夹。

**诊断**：微调模型对训练时的文件夹名称（Work, Personal, Promotions, Alerts）存在强烈偏见。

**解决方案**：将默认文件夹重命名为训练时未见过的名称（Job, Private, Deals, Notices），释放模型的涌现泛化能力。

**结果**：模型开始基于描述分类，自定义文件夹正常工作。

### 已知问题与局限性

1. **模型大小**：475MB GGUF 文件需要下载，首次安装较慢
2. **内存占用**：推理时占用约 1GB RAM
3. **首次推理延迟**：冷启动约 2.5 秒，后续 1.3 秒
4. **语义重叠**：相似描述的文件夹可能产生分类混淆
5. **中文支持**：模型对中文通知分类准确率略低于英文
