# 未来改进计划

本文档记录了超出当前竞赛范围但值得未来实现的改进想法。

---

## 性能优化

### 1. Vulkan GPU 加速
**优先级:** 高
**预期收益:** 2-3x 推理加速

利用移动端 GPU 进行矩阵运算。llama.cpp 已支持 Vulkan 后端。

**实现要点:**
- CMakeLists.txt 添加 `ggml-vulkan` 编译
- 检测设备 Vulkan 支持情况
- 设置 `model_params.n_gpu_layers` 卸载层到 GPU
- 注意：并非所有 Android 设备都支持 Vulkan 计算

**参考:** llama.cpp Vulkan 文档

### 2. 投机解码 (Speculative Decoding)
**优先级:** 中
**预期收益:** 1.5-2x 解码加速

使用小型草稿模型预测多个 token，主模型验证。

**实现要点:**
- 需要额外的小模型（如 Qwen3-0.3B 量化版）
- 增加内存占用
- 对短输出（如 JSON）收益有限

### 3. 更激进的量化
**优先级:** 低
**预期收益:** 减少内存、略微加速

当前使用 Q8_0，可测试 Q4_K_M 或 Q4_0。

**权衡:**
- 更小的模型 → 更快加载、更少内存
- 可能降低分类准确率
- 需要重新评估基准测试

---

## 功能增强

### 4. 动态文件夹学习
**优先级:** 高
**复杂度:** 中

允许用户通过修正分类来改进模型行为。

**方案 A: 提示词注入**
- 记录用户修正历史
- 在系统提示中添加 few-shot 示例
- 无需重新训练

**方案 B: 在线微调**
- 收集用户修正数据
- 定期在设备上微调 LoRA 适配器
- 需要更多工程工作

### 5. 多语言支持
**优先级:** 中
**复杂度:** 中

当前训练数据主要是英文，中文通知可能分类不准。

**实现要点:**
- 生成中文合成数据集
- 混合中英文数据训练
- 或使用多语言基础模型

### 6. 通知摘要功能
**优先级:** 低
**复杂度:** 高

批量通知时生成摘要，如"你有 5 条工作消息，2 条需要立即处理"。

**实现要点:**
- 需要更长的上下文窗口
- 可能需要更大的模型
- 增加推理时间

---

## 架构改进

### 7. 通知去重
**优先级:** 高
**复杂度:** 低

当前同一通知可能被处理两次（Android 通知监听器行为）。

**实现要点:**
- 基于 (packageName, title, body, timestamp) 生成唯一 ID
- 在 ClassificationService 中检查是否已处理
- 使用内存缓存或数据库索引

### 8. 模型预加载
**优先级:** 中
**复杂度:** 低

当前模型在首次分类时懒加载，导致首次延迟较高。

**实现要点:**
- 在 Application.onCreate() 中后台初始化模型
- 使用 WorkManager 或 CoroutineScope
- 权衡：增加启动时内存占用

### 9. 批量推理
**优先级:** 低
**复杂度:** 中

当多个通知同时到达时，批量处理而非逐个处理。

**实现要点:**
- 需要修改 llama.cpp 调用方式
- 可能需要更大的 KV cache
- 对突发通知场景有帮助

---

## 用户体验

### 10. 分类置信度显示
**优先级:** 中
**复杂度:** 中

显示模型对分类结果的置信度，低置信度时提示用户确认。

**实现要点:**
- 从 logits 计算 softmax 概率
- 需要修改 JNI 接口返回置信度
- UI 显示置信度指示器

### 11. 离线优先级学习
**优先级:** 低
**复杂度:** 高

基于用户点击行为学习优先级偏好。

**实现要点:**
- 记录用户点击顺序和时间
- 分析哪些通知被优先查看
- 调整优先级权重

### 12. 智能静音时段
**优先级:** 低
**复杂度:** 中

学习用户作息，自动调整通知行为。

**实现要点:**
- 记录用户活跃时间模式
- 在非活跃时段提升批量聚合阈值
- 紧急通知仍立即推送

---

## 技术债务

### 13. 废弃 API 迁移
**优先级:** 低
**影响:** 代码健康

llama.cpp 更新了 API，当前代码使用废弃函数：
- `llama_load_model_from_file` → `llama_model_load_from_file`
- `llama_new_context_with_model` → `llama_init_from_model`
- `llama_free_model` → `llama_model_free`
- `llama_token_is_eog` → `llama_vocab_is_eog`

### 14. 配置外部化
**优先级:** 低
**影响:** 可维护性

将硬编码值移到配置：
- 批量大小 (128)
- 上下文长度 (2048)
- 线程数 (4)
- 批量通知间隔 (30 分钟)

### 15. 单元测试覆盖
**优先级:** 中
**影响:** 代码质量

关键组件缺少测试：
- PromptBuilder 输出格式
- ClassificationParser JSON 解析
- NotificationDispatcher 优先级路由

---

## 实验性想法

### 16. 本地 RAG 增强
使用本地向量数据库存储历史通知，为分类提供上下文。

### 17. 跨设备同步
通过端到端加密同步分类偏好和修正历史。

### 18. 通知意图预测
预测用户可能的响应动作（回复、忽略、存档）。

---

## 优先级排序建议

**短期 (1-2 周):**
1. 通知去重 (#7)
2. 模型预加载 (#8)
3. 废弃 API 迁移 (#13)

**中期 (1-2 月):**
1. Vulkan GPU 加速 (#1)
2. 动态文件夹学习 (#4)
3. 分类置信度显示 (#10)

**长期 (3+ 月):**
1. 多语言支持 (#5)
2. 投机解码 (#2)
3. 本地 RAG 增强 (#16)
