cmake_minimum_required(VERSION 3.22.1)
project("notifai_llama")

set(CMAKE_C_STANDARD 11)
set(CMAKE_C_STANDARD_REQUIRED true)
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED true)

# Path to llama.cpp
set(LLAMA_CPP_DIR "${CMAKE_CURRENT_SOURCE_DIR}/../../../../../llama.cpp")

# Set 16KB page alignment for Android 15+ compatibility
set(CMAKE_SHARED_LINKER_FLAGS "${CMAKE_SHARED_LINKER_FLAGS} -Wl,-z,max-page-size=16384")

# Define version macros (normally generated by llama.cpp cmake)
# Note: LLAMA_BUILD_NUMBER, LLAMA_COMMIT, LLAMA_COMPILER, LLAMA_BUILD_TARGET
# are defined as extern variables in common.h, so we provide them in build_info.cpp
add_compile_definitions(
    GGML_VERSION="unknown"
    GGML_COMMIT="unknown"
)

# Include directories
set(LLAMA_INCLUDE_DIRS
    ${LLAMA_CPP_DIR}
    ${LLAMA_CPP_DIR}/include
    ${LLAMA_CPP_DIR}/common
    ${LLAMA_CPP_DIR}/ggml/include
    ${LLAMA_CPP_DIR}/ggml/src
    ${LLAMA_CPP_DIR}/ggml/src/ggml-cpu
    ${LLAMA_CPP_DIR}/src
    ${LLAMA_CPP_DIR}/vendor  # nlohmann/json.hpp
)

# Collect source files using glob patterns
file(GLOB GGML_SRC_C ${LLAMA_CPP_DIR}/ggml/src/*.c)
file(GLOB GGML_SRC_CPP ${LLAMA_CPP_DIR}/ggml/src/*.cpp)
file(GLOB GGML_CPU_C ${LLAMA_CPP_DIR}/ggml/src/ggml-cpu/*.c)
file(GLOB GGML_CPU_CPP ${LLAMA_CPP_DIR}/ggml/src/ggml-cpu/*.cpp)
file(GLOB LLAMA_SRC ${LLAMA_CPP_DIR}/src/*.cpp)
file(GLOB LLAMA_MODELS_SRC ${LLAMA_CPP_DIR}/src/models/*.cpp)  # Model implementations
file(GLOB COMMON_SRC ${LLAMA_CPP_DIR}/common/*.cpp)

# Core source files for llama library
set(LLAMA_SOURCE_FILES
    # GGML core
    ${GGML_SRC_C}
    ${GGML_SRC_CPP}

    # GGML CPU backend
    ${GGML_CPU_C}
    ${GGML_CPU_CPP}

    # Llama sources
    ${LLAMA_SRC}

    # Model implementations (llm_build_* classes)
    ${LLAMA_MODELS_SRC}

    # Common utilities (needed for sampling, etc.)
    ${COMMON_SRC}
)

find_library(LOG_LIB log)

# Function to build optimized library variants
function(build_llama_library target_name arch cpu_flags)
    message(STATUS "Building ${target_name} for arch=${arch} with flags=${cpu_flags}")

    # Collect architecture-specific source files
    set(ARCH_SOURCE_FILES "")
    if (NOT "${arch}" STREQUAL "generic")
        set(ARCH_DIR ${LLAMA_CPP_DIR}/ggml/src/ggml-cpu/arch/${arch})
        if (EXISTS "${ARCH_DIR}/quants.c")
            list(APPEND ARCH_SOURCE_FILES ${ARCH_DIR}/quants.c)
        endif()
        if (EXISTS "${ARCH_DIR}/repack.cpp")
            list(APPEND ARCH_SOURCE_FILES ${ARCH_DIR}/repack.cpp)
        endif()
        if (EXISTS "${ARCH_DIR}/cpu-feats.cpp")
            list(APPEND ARCH_SOURCE_FILES ${ARCH_DIR}/cpu-feats.cpp)
        endif()
        message(STATUS "  Arch-specific sources: ${ARCH_SOURCE_FILES}")
    endif()

    # Create the shared library with all sources
    add_library(${target_name} SHARED
        ${LLAMA_SOURCE_FILES}
        ${ARCH_SOURCE_FILES}
        llama_jni.cpp
        build_info.cpp
    )

    target_include_directories(${target_name} PRIVATE
        ${LLAMA_INCLUDE_DIRS}
    )

    # Base compile definitions
    target_compile_definitions(${target_name} PRIVATE
        GGML_USE_CPU=1
        NDEBUG=1
    )

    # Generic CPU flag for fallback build
    if ("${arch}" STREQUAL "generic")
        target_compile_definitions(${target_name} PRIVATE GGML_CPU_GENERIC=1)
    endif()

    # Critical performance flags (from llama.rn)
    target_compile_options(${target_name} PRIVATE
        # Optimization
        -O3
        -DNDEBUG

        # Vectorization (KEY for performance!)
        -fvectorize

        # Fast floating point
        -ffp-model=fast
        -fno-finite-math-only

        # Link-time optimization
        -flto

        # Dead code elimination
        -ffunction-sections
        -fdata-sections

        # GNU extensions
        -D_GNU_SOURCE

        # Threading
        -pthread

        # Architecture-specific flags
        ${cpu_flags}
    )

    # Linker options
    target_link_options(${target_name} PRIVATE
        -Wl,--gc-sections
        -Wl,-z,max-page-size=16384
        -flto
    )

    target_link_libraries(${target_name} PRIVATE
        ${LOG_LIB}
        android
    )
endfunction()

# Build library variants based on Android ABI
if (ANDROID_ABI STREQUAL "arm64-v8a")
    message(STATUS "Building optimized ARM64 variants for maximum performance")

    # Generic fallback (lowest common denominator)
    build_llama_library("llama_jni" "generic" "")

    # ARMv8 base (NEON guaranteed)
    build_llama_library("llama_jni_v8" "arm" "-march=armv8-a")

    # ARMv8.2 with FP16 (most modern phones)
    build_llama_library("llama_jni_v8_2" "arm" "-march=armv8.2-a")

    # ARMv8.2 with DotProd (Cortex-A75+, Pixel 3+)
    build_llama_library("llama_jni_v8_2_dotprod" "arm" "-march=armv8.2-a+dotprod")

    # ARMv8.2 with I8MM (Cortex-X1+, Pixel 6+)
    build_llama_library("llama_jni_v8_2_i8mm" "arm" "-march=armv8.2-a+i8mm")

    # ARMv8.2 with DotProd + I8MM (best performance, Pixel 6+)
    build_llama_library("llama_jni_v8_2_dotprod_i8mm" "arm" "-march=armv8.2-a+dotprod+i8mm")

elseif (ANDROID_ABI STREQUAL "x86_64")
    message(STATUS "Building x86_64 variant")
    build_llama_library("llama_jni" "generic" "")
    build_llama_library("llama_jni_x86_64" "x86" "-march=x86-64;-mtune=generic;-msse4.2;-mpopcnt")

else()
    message(STATUS "Building generic variant for ${ANDROID_ABI}")
    build_llama_library("llama_jni" "generic" "")
endif()
